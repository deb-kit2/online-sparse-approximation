{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uynstkaRrvP8",
        "remXCEbDl940",
        "P9IRLbjqjUj1",
        "_GxVN9c2jiQd"
      ],
      "authorship_tag": "ABX9TyNY2wKa5+kioRaf3g//h9ci"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "SXmMzi27l7EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "D159b9kJk4N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function"
      ],
      "metadata": {
        "id": "uynstkaRrvP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_regret(history : dict) :\n",
        "\n",
        "    running_X = history[\"running_X\"]\n",
        "    running_Y = history[\"running_Y\"]\n",
        "    phi = history[\"phi\"]\n",
        "    running_X_best = history[\"running_X_best\"]\n",
        "\n",
        "    M, N = phi.shape\n",
        "    regrets = []\n",
        "    average_regrets = []\n",
        "\n",
        "    online_loss = 0\n",
        "    x_best_ = np.zeros(N)\n",
        "    for i, (x, y, x_b) in tqdm(\n",
        "        enumerate(zip(running_X, running_Y, running_X_best)),\n",
        "        total = len(running_X)\n",
        "        ) :\n",
        "        online_loss += 0.5 * np.linalg.norm(y - phi @ x) ** 2\n",
        "\n",
        "        x_best_ += x_b\n",
        "        offline_loss = 0\n",
        "        for j in range(i + 1) :\n",
        "            offline_loss += 0.5 * np.linalg.norm(running_Y[j] - phi @ (x_best_ / (i+1)))\n",
        "\n",
        "        # print(f\"Online loss : {online_loss}, Offline Loss {offline_loss}, Regret : {online_loss - offline_loss}\")\n",
        "        regrets.append(online_loss - offline_loss)\n",
        "        average_regrets.append((online_loss - offline_loss) / (i+1))\n",
        "\n",
        "    history[\"regrets\"] = regrets\n",
        "    history[\"average_regrets\"] = average_regrets\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "KqbEF2fBrzHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTP"
      ],
      "metadata": {
        "id": "remXCEbDl940"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mW4E_GQkkZA"
      },
      "outputs": [],
      "source": [
        "def ft_hard_thresholded_pl(eta, mu, phi, K, T) :\n",
        "    \"\"\"\n",
        "    Follow the Hard-Thresholded Perturbed Leader algorithm.\n",
        "\n",
        "    Inputs :\n",
        "    eta : multiplicative factor for perturbation\n",
        "    mu : some constant > 0, preferably != 1\n",
        "    phi : shape(M, N)\n",
        "    K : sparsity constant, number of non-zero elements\n",
        "    T : timesteps to run the simulation for\n",
        "\n",
        "    Returns :\n",
        "    A dictionary containing `rewards`, `running_Y`, and `running_X`\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "    M, N = phi.shape\n",
        "    running_Y = []\n",
        "    running_X = []\n",
        "    running_X_best = []\n",
        "    rewards = []\n",
        "    support_x = np.random.permutation(N)[:K] # fixed support setting\n",
        "\n",
        "    Y = np.zeros(M)\n",
        "    gamma = np.random.normal(loc = 0.0, scale = 1.0, size = M) # constant\n",
        "\n",
        "    for t in tqdm(range(1, T + 1)) :\n",
        "        z = np.zeros(N)\n",
        "        b = (Y + eta * gamma) / t\n",
        "\n",
        "        # get x from oracle\n",
        "        tau = int(np.log(t)) + 10\n",
        "        for s in range(tau) :\n",
        "            # abs? I think needed.\n",
        "            L = np.argsort(np.abs(z + mu * phi.T @ (b - phi @ z)))[::-1][:K]\n",
        "            z_new = np.linalg.lstsq(phi[:, L], b, rcond = None)[0]\n",
        "\n",
        "            z = np.zeros(N)\n",
        "            z[L] = z_new\n",
        "\n",
        "        x = z\n",
        "\n",
        "        # reveal y\n",
        "        x_best = np.zeros(N)\n",
        "        ###### very important\n",
        "        x_best[support_x] = np.random.normal(loc = 0.0, scale = 1/5.64, size = K)\n",
        "        noise = np.random.normal(scale = 1/1024, size = M)\n",
        "\n",
        "        y = (phi @ x_best) + noise\n",
        "\n",
        "        # get reward\n",
        "        reward = np.dot(y, phi @ x) - 0.5 * np.linalg.norm(phi @ x) ** 2\n",
        "        Y = Y + y\n",
        "\n",
        "        rewards.append(reward)\n",
        "        running_X.append(x)\n",
        "        running_Y.append(y)\n",
        "        running_X_best.append(x_best)\n",
        "\n",
        "    accumulated_rewards = copy.deepcopy(rewards)\n",
        "    for i in range(1, len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] += accumulated_rewards[i-1]\n",
        "\n",
        "    for i in range(len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] /= i+1\n",
        "\n",
        "    return {\n",
        "        \"phi\" : phi,\n",
        "        \"rewards\" : rewards,\n",
        "        \"average_rewards\" : accumulated_rewards,\n",
        "        \"running_X\" : running_X,\n",
        "        \"running_X_best\" : running_X_best,\n",
        "        \"running_Y\" : running_Y\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoSaMP"
      ],
      "metadata": {
        "id": "P9IRLbjqjUj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ft_cosamp_pl(eta, phi, K, T) :\n",
        "    \"\"\"\n",
        "    Follow the CoSaMPed Perturbed Leader algorithm.\n",
        "\n",
        "    Inputs :\n",
        "    eta : multiplicative factor for perturbation\n",
        "    phi : shape(M, N)\n",
        "    K : sparsity constant, number of non-zero elements\n",
        "    T : timesteps to run the simulation for\n",
        "\n",
        "    Returns :\n",
        "    A dictionary containing `rewards`, `running_Y`, and `running_X`\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "    M, N = phi.shape\n",
        "    running_Y = []\n",
        "    running_X = []\n",
        "    running_X_best = []\n",
        "    rewards = []\n",
        "    support_x = np.random.permutation(N)[:K] # fixed support setting\n",
        "\n",
        "    Y = np.zeros(M)\n",
        "    gamma = np.random.normal(loc = 0.0, scale = 1.0, size = M) # constant\n",
        "\n",
        "    for t in tqdm(range(1, T + 1)) :\n",
        "        z = np.zeros(N)\n",
        "        b = (Y + eta * gamma) / t\n",
        "\n",
        "        # get x from oracle\n",
        "        tau = int(np.log(t) + 10)\n",
        "        for s in range(tau) :\n",
        "            # abs? I think needed.\n",
        "            sup1 = np.nonzero(z)\n",
        "            sup2 = np.argsort(np.abs(phi.T @ (b - phi @ z)))[::-1][:2*K]\n",
        "\n",
        "            L = np.union1d(sup1, sup2)\n",
        "            u = np.linalg.lstsq(phi[:, L], b, rcond = None)[0]\n",
        "\n",
        "            topK = np.argsort(np.abs(u))[::-1][:K]\n",
        "            z = np.zeros(N)\n",
        "            z[topK] = u[topK]\n",
        "\n",
        "        x = z\n",
        "\n",
        "        # reveal y\n",
        "        x_best = np.zeros(N)\n",
        "        x_best[support_x] = np.random.normal(loc = 0.0, scale = 1/5.64, size = K)\n",
        "        noise = np.random.normal(scale = 1/128, size = M)\n",
        "\n",
        "        y = (phi @ x_best) + noise\n",
        "\n",
        "        # get reward\n",
        "        reward = np.dot(y, phi @ x) - 0.5 * np.linalg.norm(phi @ x) ** 2\n",
        "        Y = Y + y\n",
        "\n",
        "        rewards.append(reward)\n",
        "        running_X.append(x)\n",
        "        running_Y.append(y)\n",
        "        running_X_best.append(x_best)\n",
        "\n",
        "    accumulated_rewards = copy.deepcopy(rewards)\n",
        "    for i in range(1, len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] += accumulated_rewards[i-1]\n",
        "\n",
        "    for i in range(len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] /= i+1\n",
        "\n",
        "    return {\n",
        "        \"phi\" : phi,\n",
        "        \"rewards\" : rewards,\n",
        "        \"average_rewards\" : accumulated_rewards,\n",
        "        \"running_X\" : running_X,\n",
        "        \"running_X_best\" : running_X_best,\n",
        "        \"running_Y\" : running_Y\n",
        "    }"
      ],
      "metadata": {
        "id": "S__qk5TyjSdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IHTP"
      ],
      "metadata": {
        "id": "_GxVN9c2jiQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ft_iter_hard_thresholded_pl(eta, mu, phi, K, T) :\n",
        "    \"\"\"\n",
        "    Follow the Iterative-Hard-Thresholded Perturbed Leader algorithm.\n",
        "\n",
        "    Inputs :\n",
        "    eta : multiplicative factor for perturbation\n",
        "    mu : some constant > 0, preferably != 1\n",
        "    phi : shape(M, N)\n",
        "    K : sparsity constant, number of non-zero elements\n",
        "    T : timesteps to run the simulation for\n",
        "\n",
        "    Returns :\n",
        "    A dictionary containing `rewards`, `running_Y`, and `running_X`\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "    M, N = phi.shape\n",
        "    running_Y = []\n",
        "    running_X = []\n",
        "    running_X_best = []\n",
        "    rewards = []\n",
        "    support_x = np.random.permutation(N)[:K] # fixed support setting\n",
        "\n",
        "    Y = np.zeros(M)\n",
        "    gamma = np.random.normal(loc = 0.0, scale = 1.0, size = M) # constant\n",
        "\n",
        "    for t in tqdm(range(1, T + 1)) :\n",
        "        z = np.zeros(N)\n",
        "        b = (Y + eta * gamma) / t\n",
        "\n",
        "        # get x from oracle\n",
        "        tau = int(np.log(t)) + 10\n",
        "        for s in range(tau) :\n",
        "            # abs? I think needed.\n",
        "            r = z + phi.T @ (b - phi @ z)\n",
        "            L = np.argsort(np.abs(r))[::-1][K]\n",
        "\n",
        "            z = np.zeros(N)\n",
        "            z[L] = r[L]\n",
        "\n",
        "        x = z\n",
        "\n",
        "        # reveal y\n",
        "        x_best = np.zeros(N)\n",
        "        ###### very important\n",
        "        x_best[support_x] = np.random.normal(loc = 0.0, scale = 1/5.64, size = K)\n",
        "        #####################\n",
        "        noise = np.random.normal(scale = 1/1024, size = M)\n",
        "\n",
        "        y = (phi @ x_best) + noise\n",
        "\n",
        "        # get reward\n",
        "        reward = np.dot(y, phi @ x) - 0.5 * np.linalg.norm(phi @ x) ** 2\n",
        "        Y = Y + y\n",
        "\n",
        "        rewards.append(reward)\n",
        "        running_X.append(x)\n",
        "        running_Y.append(y)\n",
        "        running_X_best.append(x_best)\n",
        "\n",
        "    accumulated_rewards = copy.deepcopy(rewards)\n",
        "    for i in range(1, len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] += accumulated_rewards[i-1]\n",
        "\n",
        "    for i in range(len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] /= i+1\n",
        "\n",
        "    for i in range(len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] /= i+1\n",
        "\n",
        "    return {\n",
        "        \"phi\" : phi,\n",
        "        \"rewards\" : rewards,\n",
        "        \"average_rewards\" : accumulated_rewards,\n",
        "        \"running_X\" : running_X,\n",
        "        \"running_X_best\" : running_X_best,\n",
        "        \"running_Y\" : running_Y\n",
        "    }"
      ],
      "metadata": {
        "id": "Oz4WYlrujSaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subspace Pursuit"
      ],
      "metadata": {
        "id": "ffMsp43Gjqmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual(y, A) :\n",
        "    yp = (A @ np.linalg.pinv(A)) @ y\n",
        "    res = y - yp\n",
        "    return res\n",
        "\n",
        "\n",
        "def ft_subspace_pursuit_pl(eta, phi, K, T) :\n",
        "    \"\"\"\n",
        "    Follow the Subspace Pursuit Perturbed Leader algorithm.\n",
        "\n",
        "    Inputs :\n",
        "    eta : multiplicative factor for perturbation\n",
        "    phi : shape(M, N)\n",
        "    K : sparsity constant, number of non-zero elements\n",
        "    T : timesteps to run the simulation for\n",
        "\n",
        "    Returns :\n",
        "    A dictionary containing `rewards`, `running_Y`, and `running_X`\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "    M, N = phi.shape\n",
        "    running_Y = []\n",
        "    running_X = []\n",
        "    running_X_best = []\n",
        "    rewards = []\n",
        "    support_x = np.random.permutation(N)[:K] # fixed support setting\n",
        "\n",
        "    Y = np.zeros(M)\n",
        "    gamma = np.random.normal(loc = 0.0, scale = 1.0, size = M) # constant\n",
        "\n",
        "    for t in tqdm(range(1, T + 1)) :\n",
        "        b = (Y + eta * gamma) / t\n",
        "\n",
        "        tl = np.argsort(np.abs(phi.T @ b))[::-1][:K]\n",
        "        r = residual(b, phi[:, tl])\n",
        "\n",
        "        # get x from oracle\n",
        "        tau = int(np.log(t) + 10)\n",
        "        for s in range(tau) :\n",
        "\n",
        "            sup = np.argsort(np.abs(phi.T @ r))[::-1][:K]\n",
        "            tl_ = np.union1d(tl, sup)\n",
        "\n",
        "            x_p = np.linalg.pinv(phi[:, tl_]) @ b\n",
        "\n",
        "            tl = np.argsort(np.abs(x_p))[::-1][:K]\n",
        "            r = residual(b, phi[:, tl])\n",
        "\n",
        "        x = np.zeros(N)\n",
        "        x[tl] = np.linalg.pinv(phi[:, tl]) @ b\n",
        "\n",
        "        # reveal y\n",
        "        x_best = np.zeros(N)\n",
        "        x_best[support_x] = np.random.normal(loc = 0.0, scale = 1/5.818, size = K)\n",
        "        noise = np.random.normal(scale = 1/128, size = M)\n",
        "\n",
        "        y = (phi @ x_best) + noise\n",
        "\n",
        "        # get reward\n",
        "        reward = np.dot(y, phi @ x) - 0.5 * np.linalg.norm(phi @ x) ** 2\n",
        "        Y = Y + y\n",
        "\n",
        "        rewards.append(reward)\n",
        "        running_X.append(x)\n",
        "        running_Y.append(y)\n",
        "        running_X_best.append(x_best)\n",
        "\n",
        "    accumulated_rewards = copy.deepcopy(rewards)\n",
        "    for i in range(1, len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] += accumulated_rewards[i-1]\n",
        "\n",
        "    for i in range(len(accumulated_rewards)) :\n",
        "        accumulated_rewards[i] /= i+1\n",
        "\n",
        "    return {\n",
        "        \"phi\" : phi,\n",
        "        \"rewards\" : rewards,\n",
        "        \"average_rewards\" : accumulated_rewards,\n",
        "        \"running_X\" : running_X,\n",
        "        \"running_X_best\" : running_X_best,\n",
        "        \"running_Y\" : running_Y\n",
        "    }"
      ],
      "metadata": {
        "id": "u_6lOVPRjSXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "0sXkGxPEl4G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "M = 128 # try 256 too\n",
        "N = 512\n",
        "K = 32 # keep <= 40\n",
        "T = 1000 # time steps\n",
        "\n",
        "eta = 0.2 # try others\n",
        "mu = 0.3\n",
        "\n",
        "phi = np.random.normal(loc = 0.0, scale = 1/np.sqrt(M), size = (M, N))"
      ],
      "metadata": {
        "id": "XLog_ypSl3Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = ft_cosamp_pl(eta, phi, K, T)\n",
        "alg = \"cosamp\"\n",
        "history = calculate_regret(history)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"Regret\" : history[\"regrets\"],\n",
        "        \"Average Regret\" : history[\"average_regrets\"],\n",
        "        \"Reward\" : history[\"rewards\"],\n",
        "        \"Average Reward\" : history[\"average_rewards\"]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "KXJAxOytqo58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize = (8, 5), dpi = 200)\n",
        "\n",
        "a = sns.lineplot(df[[\"Regret\"]][:200], ax = ax[0][0]);\n",
        "b = sns.lineplot(df[[\"Reward\"]][:200], ax = ax[0][1]);\n",
        "c = sns.lineplot(df[[\"Average Regret\"]][:200], ax = ax[1][0]);\n",
        "f = sns.lineplot(df[[\"Average Reward\"]][:200], ax = ax[1][1]);\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"logs/{alg}_M{M}_N{N}_K{K}_T{T}_eta{eta}_mu{mu}_scale5.818.png\")"
      ],
      "metadata": {
        "id": "0vVbZS7Haj_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFLmUW3DqX_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}